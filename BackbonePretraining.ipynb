{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Conv2D, Input, Conv2DTranspose, MaxPooling2D, concatenate, BatchNormalization, Activation, Add, Dropout, DepthwiseConv2D, Flatten, Dense\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "from tensorflow.keras import optimizers\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------- #\n",
    "#                              Important Variables                                #\n",
    "# ------------------------------------------------------------------------------- #\n",
    "data_dir = \"../../DL_data/competition_data\"\n",
    "validation_data_dir = \"../../DL_data/competition_data\"\n",
    "nrows = \"all\"                 # Set to 'all' to load the whole set\n",
    "load_validation = False      # Only load the validation images and masks??\n",
    "split_train_test = False    # Split data to train and test sets??\n",
    "data_augmentation = False   # Augment the data??\n",
    "# ------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "#load the ids  and depths of 'nrows' from the training data set\n",
    "if isinstance(nrows, int) and nrows>0:\n",
    "    train_df = pd.read_csv(data_dir+\"/train.csv\", index_col=\"id\", usecols=[0], nrows=nrows)\n",
    "    depths_df = pd.read_csv(data_dir+\"/depths.csv\", index_col=\"id\")\n",
    "    train_df = train_df.join(depths_df)\n",
    "    test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "elif isinstance(nrows, str) and nrows.upper() == \"ALL\":\n",
    "    train_df = pd.read_csv(data_dir+\"/train.csv\", index_col=\"id\", usecols=[0])\n",
    "    depths_df = pd.read_csv(data_dir+\"/depths.csv\", index_col=\"id\")\n",
    "    train_df = train_df.join(depths_df)\n",
    "    test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "else:\n",
    "    raise ValueError(\"Invalid nrows value\")\n",
    "        \n",
    "\n",
    "        \n",
    "# Function that loads the ids of 'nrows' from the validation data set\n",
    "def load_validation_data(data_dir, nrows):\n",
    "    if isinstance(nrows, int) and nrows>0:\n",
    "        valid_ids = pd.read_csv(data_dir+\"/Validation_ids.csv\", usecols=[1], nrows=nrows)\n",
    "    elif isinstance(nrows, str) and nrows.upper() == \"ALL\":\n",
    "        valid_ids = pd.read_csv(data_dir+\"/Validation_ids.csv\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid nrows value\")\n",
    "    return valid_ids.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_validation:\n",
    "    ids_to_load = load_validation_data(data_dir, nrows)\n",
    "    index_list = list(train_df.index)\n",
    "    ids_list = [index_list.index(i) for i in ids_to_load]\n",
    "    train_df = train_df.iloc[ids_list]\n",
    "else:\n",
    "    ids_to_load = train_df.index\n",
    "    \n",
    "print(\"Loading images...\")\n",
    "train_df[\"images\"] = [np.array(load_img(data_dir+\"/train/images/{}.png\".format(idx),\n",
    "                                        color_mode = \"grayscale\"))/255 for idx in tqdm_notebook(list(ids_to_load))]\n",
    "print(\"Loading masks...\")\n",
    "train_df[\"masks\"] = [np.array(load_img(data_dir+\"/train/masks/{}.png\".format(idx),\n",
    "                                       color_mode = \"grayscale\"))/65535 for idx in tqdm_notebook(list(ids_to_load))] \n",
    " #train_df.index\n",
    "\n",
    "print(\"done loading images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepossessing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Resize to a pow of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either pad with zeros or resize with interpolation\n",
    "resize_to = 128\n",
    "original_size = 101\n",
    "\n",
    "\n",
    "def upsample(original_img):\n",
    "    if resize_to == original_size:\n",
    "        return original_img\n",
    "    return resize(original_img, (resize_to, resize_to), mode='constant', preserve_range=True)\n",
    "\n",
    "\n",
    "def pad_zeros(array):\n",
    "    padded_image = np.zeros(shape=(resize_to, resize_to))\n",
    "    padded_image[13:114, 13:114] = array\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing_function_to_use = pad_zeros\n",
    "\n",
    "images_resized = np.array(train_df.images.map(resizing_function_to_use).tolist()).reshape((-1, resize_to, resize_to, 1))\n",
    "masks_resized = np.array(train_df.masks.map(resizing_function_to_use).tolist()).reshape((-1, resize_to, resize_to, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "def plot_reshape_example():\n",
    "    fig_reshape, (axs_reshape_mask, axs_reshape_img) = plt.subplots(1, 2)\n",
    "    fig_reshape.suptitle(\"Reshaped data example\")\n",
    "    axs_reshape_img.set(title=\"Reshaped image\")\n",
    "    axs_reshape_mask.set(title=\"Reshaped mask\")\n",
    "    axs_reshape_img.imshow(images_resized[id_index], cmap='gray')\n",
    "    axs_reshape_mask.imshow(masks_resized[id_index], cmap='gray')\n",
    "\n",
    "\n",
    "#plot_reshape_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the salt coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / (train_df[\"masks\"][0].shape[0]*train_df[\"masks\"][0].shape[1])\n",
    "\n",
    "# Generate salt coverage classes\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the salt coverage classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO_DO: Change that to use matplotlib\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "n_bins = 20\n",
    "axs[0].hist(train_df.coverage, bins=n_bins)\n",
    "axs[1].hist(train_df.coverage, bins=10)\n",
    "\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_train_test:\n",
    "    (ids_train, ids_valid, x_train, x_valid, y_train, y_valid,\n",
    "     cov_train, cov_test, depth_train, depth_test) = train_test_split(train_df.index.values, \n",
    "                                                                      images_resized, masks_resized, \n",
    "                                                                      train_df.coverage.values, \n",
    "                                                                      train_df.z.values, \n",
    "                                                                      test_size=0.5, \n",
    "                                                                      stratify=train_df.coverage_class,\n",
    "                                                                      random_state=1337)\n",
    "else:\n",
    "    x_train = images_resized\n",
    "    y_train = masks_resized\n",
    "    x_valid = np.array([])  # Just to print the x_valid.shape([0]) in the end\n",
    "    y_valid = np.array([]) \n",
    "\n",
    "print(\"Train/ Valid shape = %d/ %d\"%(x_train.shape[0], x_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_augmentation:\n",
    "    x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "    y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.repeat(x_train,3,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_rain shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 3)\n",
    "\n",
    "backbone = MobileNetV2(input_shape=input_shape,weights='imagenet',include_top=False)\n",
    "\n",
    "inputs = backbone.input\n",
    "\n",
    "#backbone.get_layer('block_5_depthwise').get_weights()\n",
    "\n",
    "# conv4 = backbone.output\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coverage_label = np.array(train_df[\"coverage_class\"])\n",
    "coverage_label = to_categorical(coverage_label)\n",
    "\n",
    "print(coverage_label.shape)\n",
    "\n",
    "cov_bn = backbone.get_layer('block_13_expand_relu')\n",
    "cov_bn = cov_bn.output\n",
    "cov_flatten = Flatten()(cov_bn)\n",
    "#cov_output = Dense(100, activation=\"softmax\")(cov_flatten)\n",
    "cov_output = Dense(11, activation=\"softmax\")(cov_flatten)\n",
    "\n",
    "cov_model = Model(inputs=[inputs], outputs=[cov_output])\n",
    "#print(\"BUILD MODEL\")\n",
    "cov_adam_optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "cov_model.compile(optimizer=cov_adam_optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "#print(\"COMPILE\")\n",
    "cov_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_epochs = 25\n",
    "cov_batch_len = 16\n",
    "cov_history = cov_model.fit(x_train, coverage_label, epochs=cov_epochs, shuffle=True, batch_size=cov_batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(hs={'Cov_Net': cov_history}, epochs=40, metric='loss')\n",
    "plot_history(hs={'Cov_Net': cov_history}, epochs=40, metric='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_model.save('cov_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "del cov_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KERNEL_DISPLAY_NAME",
   "language": "python",
   "name": "python_tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
